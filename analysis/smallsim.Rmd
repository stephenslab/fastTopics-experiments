---
title: EM vs. SCD in a small, simulated data set
author: Peter Carbonetto
output: workflowr::wflow_html
---

Here we perform a small experiment with simulated data to illustrate
the behaviour of the EM and SCD Poisson NMF algorithms for fitting
topic models. This example suggests that the EM updates have
difficulty with correlations between topics, even when they are quite
modest. The results also suggest that the basic variational EM for LDA
also experiences similar difficulties.

```{r knitr-opts, include=FALSE}
knitr::opts_chunk$set(comment = "#",collapse = TRUE,results = "hold",
                      fig.align = "center",dpi = 120)
```

Load the packages used in the analysis below, as well as additional
functions that we will use to simulate the data.

```{r load-pkgs, message=FALSE}
library(tm)
library(topicmodels)
library(fastTopics)
library(mvtnorm)
library(ggplot2)
library(cowplot)
source("../code/smallsim_functions.R")
```

Set the seed so that the results can be reproduced.

```{r set-seed}
set.seed(1)
```

Independent topics scenario
---------------------------

In this first example, we simulate a $100 \times 400$ counts matrix
from a multinomial topic model with $K = 6$ topics.

```{r simulate-data-1}
n <- 100
m <- 400
k <- 6
S <- 13*diag(k) - 2
F <- simulate_factors(m,k)
L <- simulate_loadings(n,k,S)
s <- simulate_sizes(n)
X <- simulate_multinom_counts(L,F,s)
X <- X[,colSums(X > 0) > 0]
```

The topic proportions for each of the 100 samples---that is, a row of
the counts matrix---are randomly drawn according to the
[correlated topic model](https://doi.org/10.1214/07-AOAS114): $\eta_i$
for row $i$ is drawn from the multivariate normal with mean zero and
covariance matrix $S$, such that $s_{kk} = 11$, $s_{jk} = -2$ for all
$j \neq k$. Generated in this way, the topic proportions tend to be
roughly orthogonal:

```{r structure-plot-1, fig.width=6, fig.height=1.5, message=FALSE}
topic_colors <- c("dodgerblue","darkorange","forestgreen","darkblue",
                  "gold","skyblue")
p1 <- simdata_structure_plot(L,topic_colors)
print(p1)
```

Here we compare two different updates: the EM updates and the sequential
coordinate descent (SCD). We perform 100 iterations of each. The model
fitting is initialized by first running 50 EM updates, with the aim of
better ensuring that the same local maximum is recovered by both runs.

```{r fit-1, results="hide", message=FALSE}
control <- list(extrapolate = FALSE,numiter = 4)
fit0 <- fit_poisson_nmf(X,k,numiter = 50,method = "em",control = control)
fit1 <- fit_poisson_nmf(X,fit0=fit0,numiter=100,method="em",control=control)
fit2 <- fit_poisson_nmf(X,fit0=fit0,numiter=100,method="scd",control=control)
fit0 <- poisson2multinom(fit0)
fit1 <- poisson2multinom(fit1)
fit2 <- poisson2multinom(fit2)
```

This next plot shows the improvement in the solution over time for the
EM and SCD updates. The Y axis shows the difference between the
current log-likelihood and the best log-likelihood achieved by the two
methods.

```{r plot-progress-1, fig.width=6, fig.height=2}
pdat <- rbind(data.frame(iter   = 1:150,
                         loglik = fit1$progress$loglik.multinom,
                         res    = fit1$progress$res,
                         method = "em"),
              data.frame(iter   = 1:150,
						 loglik = fit2$progress$loglik.multinom,
						 res    = fit2$progress$res,
						 method = "scd"))
pdat <- subset(pdat,iter >= 50)
pdat <- transform(pdat,
                  iter   = iter - 50,
                  loglik = max(loglik) - loglik + 0.1)
p2 <- ggplot(pdat,aes(x = iter,y = loglik,color = method)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values = c("dodgerblue","darkorange")) +
  labs(x = "iteration",y = "loglik difference") +
  theme_cowplot(font_size = 10)
p3 <- ggplot(pdat,aes(x = iter,y = res,color = method)) +
  geom_line(size = 0.75) +
  scale_color_manual(values = c("dodgerblue","darkorange")) +
  labs(x = "iteration",y = "max KKT residual") +
  theme_cowplot(font_size = 10)
plot_grid(p2,p3)
```

Among the two methods compared, the SCD updates progress more rapidly
toward a solution. Still, the EM updates recover the same solution
after a reasonable number of iterations. Indeed, the EM and SCD
estimates of the topic proportions are almost the same:

```{r loadings-scatterplot-1, fig.width=3, fig.height=2.5}
p4 <- loadings_scatterplot(fit1$L,fit2$L,topic_colors,"em","scd")
print(p4)
```

The maptpx R package uses a quasi-Newton-accelerated EM algorithm,
together with sequential programming techniques, to compute *maximum a
posteriori* estimates of the topic model parameters. (Note that here
we use a slightly modified version of the maptpx package that allows
initialization of the topic proportions as well as the word
frequencies. This modified version of the maptpx package can be
downloaded [here](https://github.com/stephenslab/maptpx).) Although
maptpx is not solving the same problem, it is close enough that it
benefits from a good initialization.

```{r maptpx}
maptpx0 <- maptpx::topics(X,k,shape = 1,initopics = fit0$F,omega = fit0$L,
                          tol = 0.001,tmax = 1000,ord = FALSE,verb = 0)
maptpx1 <- maptpx::topics(X,k,shape = 1,initopics = fit1$F,omega = fit1$L,
                          tol = 0.001,tmax = 1000,ord = FALSE,verb = 0)
maptpx2 <- maptpx::topics(X,k,shape = 1,initopics = fit2$F,omega = fit2$L,
                          tol = 0.001,tmax = 1000,ord = FALSE,verb = 0)
```

This next plot shows how well the estimates improve the log-posterior
at each iteration:

```{r plot-progress-maptpx, fig.width=3.25, fig.height=2}
pdat <- rbind(data.frame(iter = 1:length(maptpx0$progress),
                         y    = maptpx0$progress,
                         init = "em-50"),
              data.frame(iter = 1:length(maptpx1$progress),
						 y    = maptpx1$progress,
						 init = "em-150"),
              data.frame(iter = 1:length(maptpx2$progress),
						 y    = maptpx2$progress,
						 init = "scd"))
pdat <- transform(pdat,y = max(y) - y + 0.1)
p5 <- ggplot(pdat,aes(x = iter,y = y,color = init)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values = c("dodgerblue","darkblue","darkorange")) +
  labs(x = "iteration",y = "log-posterior difference") +
  theme_cowplot(font_size = 10)
print(p5)
```

All three runs arrive at the same solution, but maptpx finds the
solution much more quickly with the SCD better initialization, or
after running more (150) iterations of EM for Poisson NMF.

Next, we turn to the problem of fitting an LDA model to the same
data. 

We perform 400 iterations of variational EM, initializing the LDA
model fits using the MLEs computed above.

```{r fit-lda-1}
lda0 <- run_lda(X,fit0,numiter = 400)
lda1 <- run_lda(X,fit1,numiter = 400)
lda2 <- run_lda(X,fit2,numiter = 400)
```

Although the LDA fitting problem is different---we are now fitting a
(approximate) posterior distribution, and so the estimates are
approximate posterior means rather than MLE---initializing the LDA fit
using the MLEs greatly improves the LDA model fitting, as we will see
from this next plot: even after 400 iterations, variational EM without
a good initialization does not approach the quality of the LDA model
fits initialized using the EM and SCD estimates.

```{r plot-progress-lda-1, fig.width=3.25, fig.height=2.5}
pdat <- rbind(data.frame(iter = 1:400,elbo = lda0@logLiks,init = "em-20"),
              data.frame(iter = 1:400,elbo = lda1@logLiks,init = "em-120"),
              data.frame(iter = 1:400,elbo = lda2@logLiks,init = "scd"))
pdat <- transform(pdat,elbo = max(elbo) - elbo + 0.1)
p6 <- ggplot(pdat,aes(x = iter,y = elbo,color = init)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values = c("darkblue","dodgerblue","darkorange")) +
  labs(x = "iteration",y = "ELBO difference") +
  theme_cowplot(font_size = 10)
print(p6)
```

Next we will see an example in which the EM updates fail to make
reasonable progress.

```{r plots-for-paper-1, echo=FALSE, results="hide"}
ggsave("../plots/structure_plot_sim1.pdf",p1,width = 4,height = 1.25)
ggsave("../plots/progress_plot_sim1.pdf",plot_grid(p2,p3),width=5,height=1.7)
ggsave("../plots/loadings_scatterplot_sim1.pdf",p4,width = 2.5,height = 2)
ggsave("../plots/progress_plot_maptpx_sim1.pdf",p5,width = 2.9,height = 1.7)
ggsave("../plots/progress_plot_lda_sim1.pdf",p6,width = 2.5,height = 1.7)
```

Correlated topics scenario
--------------------------

The count data in this second example are simulated as before, with
one difference: by setting $s_{56} = s_{65} = 8$, the mixture
proportions for topics 5 and 6 are no longer close to orthogonal.

```{r simulate-data-2}
set.seed(1)
S[5,6] <- 8
S[6,5] <- 8
L <- simulate_loadings(n,k,S)
X <- simulate_multinom_counts(L,F,s)
X <- X[,colSums(X > 0) > 0]
```

Compare this Structure plot to the one above---there is more mixing of
topics 5 and 6:

```{r structure-plot-2, fig.width=6, fig.height=1.5, message=FALSE}
p7 <- simdata_structure_plot(L,topic_colors)
print(p7)
```

As before, we run the EM and SCD updates to fit the multinomial topic
model, with a twist that we perform another round of SCD updates after
running the EM updates. This will be explained shortly.

```{r fit-5, results="hide", message=FALSE}
fit0 <- fit_poisson_nmf(X,k,numiter = 50,method = "em",control = control)
fit1 <- fit_poisson_nmf(X,fit0=fit0,numiter=750,method="em",control=control)
fit2 <- fit_poisson_nmf(X,fit0=fit0,numiter=950,method="scd",control=control)
fit3 <- fit_poisson_nmf(X,fit0=fit1,numiter=200,method="scd",control=control)
fit1 <- fit_poisson_nmf(X,fit0=fit1,numiter=200,method="em",control=control)
fit0 <- poisson2multinom(fit0)
fit1 <- poisson2multinom(fit1)
fit2 <- poisson2multinom(fit2)
fit3 <- poisson2multinom(fit3)
```

In this second example, after initially making good progress, the EM
estimates remain far from the solution achieved by SCD even after
hundreds of EM updates. This isn't a case where the EM updates have
settled on a different local solution---the SCD updates quickly
"rescue" the EM estimates.

```{r plot-progress-2, fig.width=6, fig.height=2}
pdat <- rbind(data.frame(iter   = 1:1000,
                         loglik = fit1$progress$loglik.multinom,
                         res    = fit1$progress$res,
                         method = "em"),
              data.frame(iter   = 1:1000,
                         loglik = fit2$progress$loglik.multinom,
                         res    = fit2$progress$res,
                         method = "scd"),
              data.frame(iter   = 800:1000,
                         loglik = fit3$progress[800:1000,"loglik.multinom"],
                         res    = fit3$progress[800:1000,"res"],
  			             method = "em+scd"))
pdat <- subset(pdat,iter >= 50)
pdat <- transform(pdat,
				  iter   = iter - 50,
                  loglik = max(loglik) - loglik + 0.1)
p8 <- ggplot(pdat,aes(x = iter,y = loglik,color = method)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values = c("dodgerblue","darkorange","magenta")) +
  labs(x = "iteration",y = "loglik difference") +
  theme_cowplot(font_size = 10)
p9 <- ggplot(pdat,aes(x = iter,y = res,color = method)) +
  geom_line(size = 0.75) +
  scale_color_manual(values = c("dodgerblue","darkorange","magenta")) +
  ylim(0,10) +
  labs(x = "iteration",y = "max KKT residual") +
  theme_cowplot(font_size = 10)
plot_grid(p8,p9)
```

This large difference in likelihood is not due to a trivial difference
in solution---for example, there are many large differences in the
topic proportion estimates:

```{r loadings-scatterplot-2, fig.width=3, fig.height=2.5}
p10 <- loadings_scatterplot(fit1$L,fit2$L,topic_colors,"em","scd")
print(p10)
```

Most of the samples contributing to the large difference in likehood
are samples generated from combinations of topics 5 and 6 (the X and Y
axes in this scatterplot show the per-sample log-likelihoods):

```{r likelihood-scatterplot, fig.width=3.5, fig.height=2.5}
pdat <- data.frame(x = loglik_multinom_topic_model(X,fit1),
                   y = loglik_multinom_topic_model(X,fit2))
ggplot(pdat,aes(x = x,y = y,fill = L[,5] + L[,6])) +
  geom_point(color = "white",shape = 21,size = 1.75) +
  geom_abline(intercept = 0,slope = 1,color = "black",linetype = "dotted") +
  scale_fill_gradient(low = "skyblue",high = "orangered") +
  xlim(-700,-100) +
  ylim(-700,-100) +
  labs(x = "em",y = "scd",fill = "topic 5 + 6") +
  theme_cowplot(font_size = 10)
```

These results suggest that the EM algorithm may perform poorly in
settings where the topics are correlated, even if the correlations are
modest.

Given the difficulties faced by EM here, let's see whether
maptpx---which attempts to improve over EM with a quasi-Newton
acceleration scheme---fares better. As before, we initialize the
maptpx optimization using the estimates obtained by running the
Poisson NMF algorithms from above.

```{r maptpx-2}
maptpx0 <- maptpx::topics(X,k,shape = 1,initopics = fit0$F,omega = fit0$L,
                          tol = 1e-6,tmax = 500,ord = FALSE,verb = 0)
maptpx1 <- maptpx::topics(X,k,shape = 1,initopics = fit1$F,omega = fit1$L,
                          tol = 1e-6,tmax = 500,ord = FALSE,verb = 0)
maptpx2 <- maptpx::topics(X,k,shape = 1,initopics = fit2$F,omega = fit2$L,
                          tol = 1e-6,tmax = 500,ord = FALSE,verb = 0)
```

This plot shows the improvement in the maptpx solutions over
time. Clearly, maptpx, like EM, gets stuck in a state of very slow
progress, and benefits greatly from being initialized with the SCD
estimates to arrive at a much better solution.

```{r plot-progress-maptpx-2, fig.width=3.25, fig.height=2}
pdat <- rbind(data.frame(iter = 1:length(maptpx0$progress),
                         y    = maptpx0$progress,
                         init = "em-50"),
              data.frame(iter = 1:length(maptpx1$progress),
						 y    = maptpx1$progress,
						 init = "em-1000"),
              data.frame(iter = 1:length(maptpx2$progress),
						 y    = maptpx2$progress,
						 init = "scd"))
pdat <- transform(pdat,y = max(y) - y + 0.1)
p11 <- ggplot(pdat,aes(x = iter,y = y,color = init)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values = c("dodgerblue","darkblue","darkorange")) +
  labs(x = "iteration",y = "log-posterior difference") +
  theme_cowplot(font_size = 10)
print(p11)
```

Let's now see how this relates to fitting an LDA model:

```{r fit-lda-2}
lda0 <- run_lda(X,fit0,numiter = 400)
lda1 <- run_lda(X,fit1,numiter = 400)
lda2 <- run_lda(X,fit2,numiter = 400)
```

This plot shows the improvement in the objective (the ELBO) over time:

```{r plot-progress-lda-2, fig.width=3.25, fig.height=2.5}
pdat <- rbind(data.frame(iter = 1:400,elbo = lda0@logLiks,init = "none"),
              data.frame(iter = 1:400,elbo = lda1@logLiks,init = "em"),
              data.frame(iter = 1:400,elbo = lda2@logLiks,init = "scd"))
pdat <- transform(pdat,elbo = max(elbo) - elbo + 0.1)
p12 <- ggplot(pdat,aes(x = iter,y = elbo,color = init)) +
  geom_line(size = 0.75) +
  scale_y_continuous(trans = "log10") +
  scale_color_manual(values=c("darkblue","dodgerblue","darkorange"))+
  labs(x = "iteration",y = "ELBO difference") +
  theme_cowplot(font_size = 10)
print(p12)
```

In this second example, the SCD estimates provide by far the best
initialization of the LDA model fit.

```{r plots-for-paper-2, echo=FALSE, results="hide"}
ggsave("../plots/structure_plot_sim2.pdf",p7,width = 4,height = 1.25)
ggsave("../plots/progress_plot_sim2.pdf",plot_grid(p8,p9),width=5.5,height=1.7)
ggsave("../plots/loadings_scatterplot_sim2.pdf",p10,width = 2.5,height = 2)
ggsave("../plots/progress_plot_maptpx_sim2.pdf",p11,width = 2.9,height = 1.7)
ggsave("../plots/progress_plot_lda_sim2.pdf",p12,width = 2.5,height = 1.7)
```
